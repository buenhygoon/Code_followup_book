{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplt put, a data structure is a systematic way of organizing and accessing data, and an algorithm is a step-by-step porcedure for performing some task in a finite amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Experimental Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "start_time = time()\n",
    "run algorithm\n",
    "end_time = time()\n",
    "elapsed = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but it is by no means perfect. The time function measure relative to what is known as the \"wall clock\". Because many processes share use of a computer's cpu, the elapsed time will depend on what other processes are running on the computer when the test is performed.\n",
    "- Python includes a more advanced module, named timeit, to help automate such evaluations with repetition to account for such variance among trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Challenge of Experimental Analysis **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- While experimental studies of running times are valuable, especially when fine-tuning production-quality code, there are three major limitations to their use for algorithm analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experimental running times of two algorithms are difficult to directly compare unless the experiments are performed in the same hardware and software environments.\n",
    "- Experiments can be done only on a limited set of test inputs; hence, they leave out the running times of inputs not inlcuded in the experiment\n",
    "- An algorithm must be fully implemented in order to execute it to study its running time experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This last requirement is the most serious drawback to the use of experimental studies. At early stages of design, when considering a choice of data structures or algorithms, it would be foolish to spend a significant amount of time implementing an approach that could easily be deemed inferior by a higher-level analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Moving Beyong Experimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our goal is to develop an approach to analyzing the efficiency of algorithms that: \n",
    "1. Allows us to evaluate the relative efficiency of any two algorithms in a way that is independent of the hardware and software environment.\n",
    "2. Is performed by studying a high-level description of the algorithm without need for implementation.\n",
    "3. Takes into account all possible inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Counting Primitive Operations **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To analyze the running time of an algorithm without performing experiments, we perform an analysis directly on a high-level description of the algorithm. We define a set of primitive operations such as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assigning an identifier to an obeject \n",
    "- Determining the object associated with an identifier \n",
    "- Performing an arithmetic operation \n",
    "- Comparing two numbers\n",
    "- Accessing a single element of a Python list by index\n",
    "- Calling a function \n",
    "- Returning from a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formally, a primitive operation corresponds to a low-level instruction with an execution time that is constant. Ideally, this might be the type of basic operation that is executed by the hardware, although many of our primitive operations may be translated to a small number of instructions. Instead of trying to determine the specific execution time of each primitive operation, we will simply count how many primitive operations are executed, and use this number t as a measure of the running time of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This operation count will correlate to an actual running time in a specific computer, for each primitive operation corresponds to a constant number of instructions, and there are only a fixed number of primitive operations. The implicit assumption in this approach is that the running times of different primitive operations will be fairly similar. Thus, the number, t, of primitive operations an algorithm performs will be proportional to the actual running time of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Measuring Operations as a Function of Input Size **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To capture the order of growth of an algorithm's running time, we will associate with each algorithm, a function f(n) that characterizes the number of primitive operations that are performed as a function of the input size n. Section 3.2 will introduce the seven most common functions that arise, and Section 3.3 will introduce a mathematical framework for coparing functions to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Focusing on the Worst-Case Input **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An algorithm may run faster on some inputs that it does on others of the same size. Thus, we may wish to express the running time of an algorithm as the function of the input size obtained by taking the average over all possible inputs of the same size. Unfortunately, such an average-case analysis is typically quite challenging. It requires us to define a probability distribution on the set of inputs, which is often a difficult task. Figure 3.2 schematically shows how, depending on the input distribution, the running time of an algorithm can be anywhere between the worst-case time and the best-case time. For example, what if inputs are really only of types \"A\" or \"D\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An average-case analysis usually requires that we calculate expected running times based on a given input distribution, which usually involves sophisticated probability theory. Therefore, for the remainder of this book, unless we specify otherwise, we will characterize running times in terms of worst-case, as a function of the input size, n, of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Worst-case analysis is much easier than average-case analysis, as it requires only the ability to identify the worst-case input, which is often simple. Also, this approach typically leads to better algorithms. Making the stadard of success for an algorithm to perform well in the worst case necessarily requires that it will do well on every input. That is, designing for the worst case leads to stronger algoorithmic \"muscles,\" much like a track star who always pratices by running up an incline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Seven Functions Used in This Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Constant Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest function we can think of is the constant function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(n) = c, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for some fixed constant c, such as c = 5, c = 27, or c = $ 2^{10} $. That is, for any argument n, the constant function f(n) assigns the value c. In other wordsm it does not matter what the value of n isl f(n) will always be equal to the constant value c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because we are most interested in integer functions, the most fundamental constant function is g(n) = 1, and this is the typical constant function we use in this book. Note that any other constant function, f(n) = c, can be written as a constant c times g(b). That is, f(n) = cg(n) in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Logarithm Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Linear Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** THe N-Log-N Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Quadratic Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nested Loops and the Quadratic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Cubic Function and Other Polynomials **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coefficients, degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Summations **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** THe exponential Function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base, exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Geometric Sums **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Comparing Growth Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![main](classes_of_functions.png \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideally, we would like data structure operations to run  in times proportaional to the constant or logarithm function, and we would like our algorithms to run in linear or n-log-n time. Algorithms with quadratic or cubic running times are less practical, and algorithms with exponential running times are infeasible for all but the smallest sized inputs. Plots of the seven functions are shown in Figure 3.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The ceiling and Floor Functions **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One additional comment concerning the functions above is in order. When discussing logarithms, we noted that the value is generally not an integer, yet the running time of algorithm is usually expressed by means of an integer quatity, such as the number of operations performed. Thus, the analysis of algorithms may sometimes involve the use of the floor function and ceiling function, which are defined respectively as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Asymptotic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In algorithm analysis, we focus on the growth rate of the running time as a function of the input size n, taking a \"big-picture\" approach. For example, it is often enough just to know that the running time of an algorithm grows porportionally to n. \n",
    "- We analyze algorithms using a mathematical notation for functions that disregards constant factors. Namely, we characterize the running times of algorithms by using functions that map the size of the input, n, to values that correspond to the main factor that determines the growth rate in terms of n. This approach reflects that each basic step in a pseudo-code description or a high-level language implementation may correspond to a small number of primitive operations. Thus, we can perform an analysis of an algorithm by estimating the number of primitive operations executed up to a constant factor, rather than getting bogged down in language-specific or hardware-specific analysis if the exact number of operations that execute on the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a tangible example, we rivisit the goal of finding the largest element of a Python listl we first used this example when introducing for loops on page 21 of Section 1.4.2. Code Fragment 3.1 presents a function named find\\_max for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_max(data):\n",
    "    \"\"\"Return the maximum element from a nonempty Python list.\"\"\"\n",
    "    biggest = data[0]\n",
    "    for val in data:\n",
    "        if val > biggest:\n",
    "            biggest = val\n",
    "    return biggest\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a classic example of an algorithm with a running times that grows proportional to n, as the loop executes once for each data element, with some fixed number of primitive operations executing for each pass. In the remainder of this section, we provide a framework to formalize this claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 The \"Big-Oh\" Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![main](bigoh_function.png \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The big-Oh notation allows us to say that a function f(n) is \"less than or equal to\" another function g(n) up to a constant factor and in the asymptotic sense as n gorws toward infinity. This ability comes from the fact that the definition uses \"<=\" to compare f(n) to a g(n) times a constant, c, for the asymtotic cases when n >= $n_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, it is considered poor taste to say \"f(n) <= O(g(n)),\" since the big-Og already denotes the \"less-than-or-equal-to\" concept. Likewise, although common, it is not fully correct to say \"f(n) = O(g(n)),\" with the usual understanding of  the \"=\" relation, because there is no way to make sense of the symmetric statement, \"O(g(n)) = f(n).\" It is besst to say,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** f(n) is O(g(n)).\" **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Characterizing Running Times Using the Big Oh Notation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The big-Oh notation is used widely to characterize running time and space bounds in terms of some parameter n, which varies from problem to problem, but is always defined as a chosen measure of the \"size\" of the problem. For example, if we are interested in finding the largest element in a sequence, as with the find\\_max algorithm, we should let n denote the number of elements in that collection. Using the big-Oh notation, we can write the following methematically precise statement on the running time of algorithm find\\_max for any computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Characterizing Functions in Simplest Terms **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- without constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Big-Omega **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just as the big-Oh notation provides an asymptotic way of saying that a function is \"less than or equal to\" another function, the following notations provide an asymptotic way of saying that a function grows at a rate that is \"greater than or equal to\" that of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Big-theta **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, there is a notation that allows us to say that two functions grow at the same rate, up to constant factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose two algorithms solving the same problem are available: an algorithm A, which has a running time of O(n), and an algorithm B, which has a running time of O($n^{2}$). Which algorithm is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Some of Words of Caution **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A few words of caution about asymptotic notation are in order at this point. First, note that the use of the big-Oh and related notations can be somewhat misleading should the constant factors they \"hide\" be very large. For example, while it is true that the function 10000 n is O(n), if this is the running time of an algorithm being compared to one whose running time is 10nlogn, we should prefer the O(nlogn)- time algorithm, even though the linear-time algorithm is asymptotically faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exponential Running Times **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Examples of Algorithm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have the big-Oh notation for doing algorithm analysis, let us giev some examples by characterizing the running time of some simple algorithms using this notation. Moreover, in keeping with our earlier promise, we illustrate below how each of the seven functions given earlier in this chapter can be used to characterize the running time of an example algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rather than use pseudo-code in this section, we give complete Python implementations for our examples. We use Python's list class as the natural representation for an \"array\" of values. In Chapter 5, we will fully explore the underpinnings of Python's list class, and the efficiency of the various behaviors that it supports. In this section, we rely on just a few of its behaviors, discussing their efficiencies as introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Constant-Time Operations **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given an instance, named data, of  the Python list class, a call to the function len(data), is evaluated in constant time. This is a very simple algorithm because the list class maintains, for each list, and instance variable that records the current length of the list. This allows it to immediately report that length, rather than take time to iteratively count each of the elements in the list. Using asymptotic notation, we say that this function runs in I(1) time; that is, the running time of this function is independent of  the length, n, of the list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another central behavior of Python's list class is that it allows access to an arbitrary element of the list using syntax, data[j], for integer index j. Because Python's lists are implemented as array-based sequences, references to a list's elements are stored in a iterating through the list one element at a time, but by validating the index, and using it as an offset into the underlying array. In turn, computer hardware supports constant-time access to an element based on its memory address. Therefore, we say that the expression data[j] is evaluated in O(1) time for a Python list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Revisiting the Problem of Finding the Maximum of a Sequence **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prefix Averages **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next problme we consider is computing what are known as *prefix averages* of a sequence of numbers. Namely,, given a sequence S containing of n numbers, we want to compute a sequence A such that A[j] is the average of elements S[0], ..., S[j] for j = 0, ...,n 01, that is .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computing prefix averages has many applications in economics and statistics. For example, given the year-by-year returns of a mutula fund, ordered from recent to past, an investor will typically want to see the fund's average annual returns for the most recent year, the most recent three years, the most recent five years, and so on. Likewise, given a stream of daily Web usage logs, a Web site manager may wish to track average usage tredns over various time periods. We analyze three different implementations that solve this problem but with rather different running times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A Quadratic-Time Algorithm **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our first algorithm for computing prefix averages, named prefix\\_average1, is shown in Code Fragment 3.2. It computes every element of A separately, using an inner loop to compute the partila sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_avearge1(S):\n",
    "    \"\"\"Return list such that, for all j, A[j] equals average of S[0], ..., S[j]\"\"\"\n",
    "    n = len(S)\n",
    "    A = [0] * n\n",
    "    for j in range(n):\n",
    "        total = 0 \n",
    "        for i in range(j + 1):\n",
    "            total += S[i]\n",
    "        A[j] = total / (j + 1)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_average2(S):\n",
    "    \"\"\"Return list such that, for all j, A[j] equals average of S[0], ..., S[j]\"\"\"\n",
    "    n = len(S)\n",
    "    A = [0] * n\n",
    "    for j in range(n):\n",
    "        A[j] = sum(S[0:j+1]) / (j + 1)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This approach is essentially the same high-level algorithm as in prefix\\_average1, but we have replaced the inner loop by using the single expression sum(S[0:j+1]) to compute the partial sum, S[0] + ... + S[j]. While the use of that function greatly simplifies the presentation of the algorithm, it is worth asking how it affects the efficiency. Asymptotically, this implementation is no better. Even though the expression, sum(S[0:j+1]), seems like a single command, it is a function call and an evaluation of that function takes O(j+1) time in this context. Technically, the computation of the slice, S[0:j+1], also uses O(j+1) time, as it constructs a new list instance for storage. So the running time of prefix\\_average2 is still dominated by a series of steps that take time proportional to 1 + 2 ... and thus O($n^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear-Time Algorithm **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our final algorithm, prefix\\)average3, is given in Code Fragment 3.4. Just as with our first two algorithms, we are interested in computing, for each j, the *prefix sum* S[0] + S[1] + ... + S[j], denoted as total in our code, so that we can then compute the prefix average A[j] = total / (j + 1). However, there is a key difference that results in much greater efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_average3(S):\n",
    "    \"\"\"Return list such that, for all j, A[[j] equals average of S[0],..., S[j]\"\"\"\n",
    "    n = len(S)\n",
    "    A = [0] * n\n",
    "    total = 0\n",
    "    for j in range(n):\n",
    "        total += S[j]\n",
    "        A[j] = total / (j + 1)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Three-Way Set Disjointness **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we are given three sequences of numbers, A, B and C. We will assume that no individual sequence contains duplicate values, but that there may be some numbers taht are in two or three of the sequences. The **three-way set disjointness** problem is to determine if the intersection of the three sequence is empty, namely, that there is no element x such that ... A simple Python function to determine this property is given in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disjoint1(A, B, C):\n",
    "    \"\"\"Return True if there is no element common to all three lists.\"\"\"\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            for c in C:\n",
    "                if a == b == c:\n",
    "                    return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This simple algorithm loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the originla sets has size n, then the worst-case running time of this function is O($n^3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can improve upon the asymptotic performance with a simple observation. Once inside the body of the loop over B, if selected elements a and b do not match each other, it is a waste of time to iterate through all values of C looking for a matching triple. An improved solution to this problem, taking advantage of this observation, is presented in Code Fragment 3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint2(A, B, C):\n",
    "    \"\"\"Return True if there is no element common to all three lists.\"\"\"\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            if a == b:\n",
    "                for a in c:\n",
    "                    if a == c:\n",
    "                        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the improved version, it is not simply that we save time if we get lucky. WE claim that the worst-case running time for disjoint2 is O($n^2$). There are quadratically many pairs (a, b) to consider. However, if A and B are each sets of distinct elements, there can be at most O(n) such pairs with a equal to b. Therefore, the innermost loop, over C, executes at most n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To account for the overall running time, we examine the time spent executing each line of code. The management of the for loop over A requires O(n) time. The management of the for loop over B accounts for a total of O($n^2$) times, since that loop is executed n different times. The test a == b is evaluated O($n^2$) times. The rest of the time spent depends upon how many matching (a, b) pairs exist. As we have noted, there are at most n such pairs, and so the management of the loop over C, and the commands within the body of that loop, use at most O($n^2$) time. By our standard application of Proposition 3.9, the total time spent is O($n^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Element Uniqueness **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A problem that is closely related to the three-way set disjointness problem is the element uniqueness problem. In the former, we are given three collections and we presumes that there were no duplicates within a single collection. In the element uniqueness problem, we are given a single sequence S with n elements and asked whether all elements of that collection are distinct from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our first solution to this problem uses a straightforward iterative algorithm. The unique1 function, given in Code Fragment 3.7, solves the element uniqueness problem by looping through all distinct pairs of indices j < k, checking if any of those pairs refer to elements that are equivalent to each other. It does this using two nested for loops, such that the first iteration of the outer loop causes n - 1 iterations of the inner loop, the second iteration of the outer loop causes n - 2 interations of the inner loop,, and so on. Thus, the worst-case running time of this function is proportional to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique1(S):\n",
    "    \"\"\"Return True if there are no duplicate elements in sequence S.\"\"\"\n",
    "    for j in range(len(S)):\n",
    "        for k in range(j+1, len(S)):\n",
    "            if S[j] == S[k]:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using Sorting as a Problem-Solving Tool **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An even better algorithm for the element uniqueness problem is based on using sorting as a problem-solving tool. In this case, by sorting the sequence of elements, we are guaranteed that any duplicate elements will be placed next to each other. Thus, to determine if there are any duplicates, all we need to do is perform a single pass over the sorted sequece, looking for *consective* duplicates. A Python implementation of this algorithm is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique2(S):\n",
    "    \"\"\"Return True if there are no duplicate elements in sequence S.\"\"\"\n",
    "    temp = sorted(S)\n",
    "    for j in range(1, len(temp)):\n",
    "        if S[j-1] == S[j]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entirely, O(nlogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Simple Justification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes, we will want to make claims about an algorithm, such as showing that it is correct or that it runs fast. In order to rigoroously make such claims, we must use mathematical language, and in order to back up such claims, we must justify or prove our statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 By Example\n",
    "- counterexample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 The \"Contra\" Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contrapositive, contradiction (DeMorgan's Law)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Induction and Loop Invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** induction **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loop Invariants **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find(S, val):\n",
    "    \"\"\"Return index j such that S[j] == val, or -1 if no such element.\"\"\"\n",
    "    n = len(S)\n",
    "    j = 0\n",
    "    while j < n:\n",
    "        if S[j] == val:\n",
    "            return j\n",
    "        j += 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
