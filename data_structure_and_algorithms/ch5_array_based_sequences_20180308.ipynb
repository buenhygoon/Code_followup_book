{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.1 Python's Sequence Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this chapter, we explore Python's various \"sequence\" classes, namely the built-in list, tuple, and str classes. There is significant commonality between these classes, most notably: each supports indexing to access an individual element of sequence, using syntax such as seq[k], and each uses a low-level comcept knwon as an array to represent the sequence. However, there are significant differences in the abstractions that these classes represent, and in the way that instances of these classes are represented internally by Python. Because these classes are used so widely in PYthon programs, and because they will become building blocks upon which we will develop more complex data structures, it is imperative that we establish a clear understanding of both the public behavior and inner workings of these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Public Behaviors **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A proper understanding of  the outward semantics for a class is a necessity for a good programmer. While the basic usage of lists, strings, and tuples may seem straightforward, there are several important subtleties regarding the behaviors associatedd with these classes (such as what it means to make a copy of a sequence, or to take a slice of a sequence). Having a misunderstanding of a behavior can easily lead to inadvertent bugs in a program. Therefore, we establish an accurate mental model for each of these classes. These images will help when exploring more advanced usage, such as representing a multidimensional data set as a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Implementation Details **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A focus on the internal implementations of these classes seems to go against our stated principles of object-oriented programming. In Section 2.1.2, we emphasized the principle of encapsulation, noting that the user of a class need not know about the internal details of the implementations. While it is true that one only needs to understand the syntax and semantics of class's public interface in order to be able to write legal and corrct code that uses instances of the class, the efficiency of a program depends greatly on the efficiency of the components which it relies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Asymptotic and Experimental Analyses **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In describing the efficiency of various operations for Python's sequence classes, we will rely on the formal asymptotic analysis notations established on Chapter 3. We will also perform experimental analyses of the primary operations to provide empirical evidence that is consistent with the more theoretical asymptotic analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Low-Level Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To accurately describe the way in which Python represents the sequence types, we must first discuss aspects of the low-level computer architecture. The primary memory of a computer is composed of bits of information, and those bits are typically grouped into larger units that depend upon the precise system architecture. Such a typical unit is a byte which is equivalent to 8 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A computer system will have a huge number of bytes of memory, and to keep track of what information is stored in what byte, the computer uses an abstraction known as a memory address. In effect, each byte of memory is associated with a unique number that serves as its address(more formally, the binary representation of the number serves as the address). In this way, the computer system can refer to the data in \"byte #2150\" versus the data in \"byte #2157\", for example. Memory addresses are typically coordinated with the physical layout of the memory system, and so we often portray the numbers in sequential fashion. Figure 5.1 provides such a diagram, with the designated memory address for each byte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Despite the sequential nature of the numbering system, computer hardware is designed, in theory, so that any byte of the main memory can be efficiently accessed based upon its memory address. Im this sense, we say that computer's main memory performs as random access memory(RAM). That is, it is just as easy to retrieve byte #8675309 as it is to retrieve byte #309.(In practice, there are complicating factors including the use of caches and external memory; we address some of those issues in Chapter 15.) Using the notation for asymptotic analysis, we say that any individual byte of memory can be stored or rerieved in O(1) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, a programming language keeps track of the association between an identifier and the memory address in which the associated value is stored. For example, identifier x might be associated with one value stored in memory, while y is associated with another value stoed in memory. A common programming task is to keep track of a sequence of related objects. For example, we may want a video game to keep track of the top ten scores for that game. Rather than use ten different variables for this task, we would prefer to use a single name for the group and use index numbers to refer to the high scores in that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A group of related variables can be stored one after another in contiguous portion of the computer's memory. We will denote such a representation as an array. As a tangible example, a text string is stored as an oredered sequence of individual characters. In Python, each character is represented using the Uniode character set, and on most computing systems, Python internally represents each Unicode character with 16 bits(i.e., 2 bytes). Therefore, a six-character string, such as \"SAMPLE\", would be stored in 12 consecutive bytes of memory, as diagrammed in Figure 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We describe this as an array of six characters, even though it requires 12 bytes of memory. We will refer to each location within an array as a cell, and will use an integer index to describe its location within the array, with cells numbered starting with 0, 1, 2, and so on. For example, in Figure 5.2, the cell of the array with index 4 has contents L and is stored in bytes 2154 and 2155 of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each cell of an array must use the same number of bytes. This requirement is what allows an arbitrary cell of the array to be accessed in constant time based on its index. In particular, if one knows the memory address at which an array starts (e.g., 2146 in Figure 5.2), the number of bytes per element (e.g. 2 for  a Unicode character), and a desired index within the array, the appropriate memory address can be computed using the calculation, start + cellsize \\* index.  By this formula, the cell at index 0 begins precisely at the start of the array, the cell at index 1 begins precisely cellsize bytes beyond the start of the array, and so on. As on example, cell 4 of Figure 5.2 begins at memory location 2146 + 2 \\* 4 = 2146 + 8 = 2154."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Of course, the arithmetic for calculating memory addresses within an array can be handled automatically. Therefore, a programmer can envision a more typical high-level abstraction of an array of characters as diagrammed in Figure 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Referential Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AS another motivating example, assume that we want a medical information system to keep track of the patients currently assigned to beds in a certain hospital. If we assume that the hospital has 200 beds, and converniently that those beds are numbered from 0 to 199, we mgiht consider using an array-based structure to maintain the names of the patients currently assigned to those beds. For example, in Python we might use a list of names, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Rene', 'Joseph', 'Janet', 'Jonas', 'Helen', 'Virginia', ///]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To represent such a list with an array, Python must adhere to the requirement that each cell of the array use the same number of bytes. Yet the elements are strings, and strings naturally have different lengths. Python could attempt to reserve enough space for each cell to hold the maximum length string (not just of currently stored strings, but of any string we might ever want to store), but that would be wasteful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead, Python represents a list or tuple instance using an internal storage mechanism of an array of object references. At the lowest level, what is stored is a consecutive sequence of memory addresses at which the elements of the sequence reside. A high-level diagram of such a list is shown in Figure 5.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the relative size of the individual elements may vary, the number of its used to store the memoty address of each element is fixed (e.g., 64-bits per address). In this way, Python can support contant-time access to a list or tuple element based on its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Figure 5.4, we characterize a list of strings that are the names of the patients in a hospital. Is is more likely that a medical information system would manage more comprehensive information on each patient, perhaps represented as an instance of a Patient class. From the perspective of the list implementation, the same principle applies: The list will simply kepp a sequence of references to those objects. Note as well that a refenrence to the None object can be used as an element of the list to represent an empty bed in the hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fact that list and tuples are referential structures is significant to the semantics of these classes. A single list instance may include multiple references to the same object as elements of the list, and it is possible for a single object to be an element of two or more lists, as those lists simply store references back to that object. As an example, when you compute a slice of a list, the result is a new list instance, but that new list has references to the same elements that are in the original list, as portrayed in Figure 5.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the elements of the list are immutable objects, as with the integer instances in Figure 5.5, the fact that the two lists share elements is not that significant, as neither of the lists can cause a change to the shared object. If, for example, the commnad temp[2] = 15 were executed from this configuration, that does not change the existing integer object; it changes the reference in cell 2 of the temp list to reference a different object. The resulting configuration is shown in Figure 5.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The same semantics is demonstrated when making a new list as a copy of an existing one, with a syntax such as backup = list(primes). This produces a new list that is a shallow copy, in that it references the same elements as in the first list. With immutable elements, this point is moot. If the contents of the list were of a mutable type, a deep copy, meaning a new list with new elements, can be produced by using the deppcopy function from the copy module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AS a more striking example, it is a common practice in Python to initialize an array of integers using a syntax such as counters = [0] \\* 8. This syntax produces a list of length eight, with all eight elements being the value zero. Technically, all eight cells of the list reference the same object, as portrayed in Figure 5.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At first glance, the extreme level of aliasing in this configuration may seem alarming. However, we rely on the fact that the referenced integer is immutable. Even a commnad such as counters[2] += 1 does not technically change the value of the existing integer insatance. This computes a new integer, with value 0 + , and sets cell 2 to reference the newly computed value. The resulting configuration is shown in Figure 5.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![main](no_new_elements.png \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AS a final manifestation of the referential nature of lists, we note that the extend commnad is used to add all elements from one list to the end of another list. The extended list does not receive copies of those elements, is receives references to those elements. Figure 5.8 portrays the effect of a call to extend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.2.2 Compact Array in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the introduction to this section, we emphasized that strings are represented using an array of characters (not an array of references). We will refer to this more direct representation as a compact array because the array is storing the bits that represent the primary data (characters, in the case of strings). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compact arrays have several advantages over referential structures in terms of computing performance. Most significantly, the overall memory usage will be much lower for a compact structure because there is no overhead devoted to the explicit storage of the sequence of memory references (in addition to the primary data). That is, a referential structure will typically use 64-bits for the memory address stored in the array, on top of whatever number of bits are used to represent the object that is considered the element. Also, each Unicode character stored in a compact array within a string typicallt requires 2 bytes. If each character were stored independently as a one-character string, there would be significantly more bytes used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As another case study, suppose we wish to store a sequence of one million, 64-bit integers. In theory, we might hope to use only 64 million bits. However, we estimate that a Python list will use four to five times as much memory. Each element of the list will result in a 64-bit memory address being stored in the primary array, and an int instance being stored elsewhere in memory. Python allows you to query the actual number of bytes being used for the primary storage of any object. This is done using the getsizeof function of the sys module. On our system, the size of a typical int object requires 14 bytes of memory (well beyond the 4 bytes needed for representing the actual 64-bit number). In all, the list will be using 18 bytes per entry, rather than the 4 bytes that a compact list of integers would require.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another important advantage to a compacy structure for high-performance computing is that the primary data are stored consecutively in memory. Note well that this is not the case for a referential structure. That is, even though a list maintains careful oerdring of the sequence of memory addresses, where those elements reside in memory is not determined by the list. Because of the working of the cache and memory hierarchies of computers, it is often acvantageous to have data stored in memory near other data that might be used in the same computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Despite the apparent inefficiencies of referential structures, we will generally be content with the convenience of Python's list and tuples in this book. The only place in which we consider alternatives will be in Chapter 15, which focuses on the impact of memory usage on data structures and algorithms. Python provides several means for creating compact arrays of various types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primary support for compact arrays is in a module named array. That module defines a class, also named array, providing compact storage for arrays of primitive data types. A portrayal of such an array of integers is shown Figure 5.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The public interface for the array class conforms mostly to that of a Python list. However, the constructor for the array class requires a type code as a first parameter, which is a character that designate the type of data that will be stored in the array. As a tangible example, the type code, 'i', designates an array of (signed) integers, typically represented using at least 16-bits eahc. We can declare the array shown in Figure 5.10 as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e916f6fae7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "primes = array('i', [2,3,5,7,11,13,17,19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The type code allows the interpreter to determine precisely how many bits are needed per element of the array. The type codes suppoted by the array module, as shown in Table 5.1, are formally based upon the native data types used by the C programming language (the language in which the most widely used distribution of Python is implemented). The precise number of bits for the C data types is system-dependent, but typical ranges are shown in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The array module does not provide support for making compact arrays of user-defined data types. Compact arrays of such structures can be created with the lower-level support of a module named ctypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Dynamic Arrays and Amortization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When creating a low-level array in a computer system, the precise size of that array must be explicitly declared in order for the system to properly allocate a consecutive piece of memory for tis storage. For example, Figure 5.11 displays an array of 12 bytes that might be stored in memory locations 2146 through 2157."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because the system might dedicate neighboring memory locations to store other data, the capacity of an array cannot trivially be incresed by expanding into subsequent cells. In the context of representing a Python tuple or str instance, this constraint is no problem. Instances of those classes are immutable, so the correct size for an underlying array can be fixed when the object is instantiated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python's list class presents a more interesting abstraction. Although a list has a particular length when constructed, the class allows us to add elements to the list, with no apparent limit on the overall capacity of the list. To provide this abstraction, Python relies on an algorithmic sleight of hand known as a dynamic array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first key to providing the semantics of a dynamic array is that a list instance maintains an underlying array that often has greater capacity than the current length of the list. For example, while a user may have created a list with five elements, the system may have reserved an underlying array capable of storing eight object references (rather than only five). This extra capacity makes it easy to append a new element to the list by using the next available cell of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If a user continues to append elements to a list, any reserved capacity will eventually be exhausted. In that case, the class requests a new, larger array from the system, and initializes the new array so that its prefix matches that of the existing smaller array. At that point in time, the old array is no longer needed, so it is reclaimed by the system. Intuitively, this strategy is much like that of the hermit crab, which moves into a larger shell when it outgrows its previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We give empirical evidence that Python's list class is based upon such a strategy. The source code for our experiment is displayed in Code Fragment 5.1, and a sample output of that program is given in Code Fragment 5.2. We rely on a function named getsizeof that is available from the sys module. This function reports the number of bytes that are being used to store an object in Python. For a list, it reports the number of bytes devoted to the array and other instance variables of the list, but not any space devoted to elements referenced by the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:   0; Size in bytes:   64\n",
      "Length:   1; Size in bytes:   96\n",
      "Length:   2; Size in bytes:   96\n",
      "Length:   3; Size in bytes:   96\n",
      "Length:   4; Size in bytes:   96\n",
      "Length:   5; Size in bytes:  128\n",
      "Length:   6; Size in bytes:  128\n",
      "Length:   7; Size in bytes:  128\n",
      "Length:   8; Size in bytes:  128\n",
      "Length:   9; Size in bytes:  192\n",
      "Length:  10; Size in bytes:  192\n",
      "Length:  11; Size in bytes:  192\n",
      "Length:  12; Size in bytes:  192\n",
      "Length:  13; Size in bytes:  192\n",
      "Length:  14; Size in bytes:  192\n",
      "Length:  15; Size in bytes:  192\n",
      "Length:  16; Size in bytes:  192\n",
      "Length:  17; Size in bytes:  264\n",
      "Length:  18; Size in bytes:  264\n",
      "Length:  19; Size in bytes:  264\n",
      "Length:  20; Size in bytes:  264\n",
      "Length:  21; Size in bytes:  264\n",
      "Length:  22; Size in bytes:  264\n",
      "Length:  23; Size in bytes:  264\n",
      "Length:  24; Size in bytes:  264\n",
      "Length:  25; Size in bytes:  264\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "data = []\n",
    "for k in range(26):\n",
    "    a = len(data)\n",
    "    b = sys.getsizeof(data)\n",
    "    print('Length: {0:3d}; Size in bytes: {1:4d}'.format(a, b))\n",
    "    data.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In evaluating the results of the experiment, we draw attention to the first line of output from Code Fragment 5.2. We see that an empty list instance already requires a certain number of bytes of memory (72 on our system). In fact, each object in Python maintains some state, for example, a reference to denote the class to which it belongs. Although we cannot directly access private instance variable for a list, we can speculate that in some form it maintains state informantion akin to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As soon as the first element is inserted into the list, we detect a change in the underlying size of the structure. In particular, we see the number of bytes jump from 72 to 104, an increase of exactly 32 bytes. Our experiment was run on a 64-bit machine architecture, meaning that each memory address is a 64-bit number(i.e., 8 bytes). We speculate that the increase of 32 bytes reflects the allocation of an underlying array capable of storing four object references. This hypothesis is consistent with the fact that we do not see any underlying change in the memory usage after inserting the second, third, or fourth element into the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the fifth element has been added to the list, we see the memory usage jump from 104 bytes to 136 bytes. If we assume the original base usage of 72 bytes for the list, the total of 136 suggests an additional 64 = 8 x 8 bytes that provide capacity for up to eight object references. Again, this is consistent with the experiment, as the memory usage does not increase again until the ninth insertion. At that point, the 200 bytes can be viewed as the original 72 plus an additional 128-byte array to store 16 object references. The 17th insertion pushes the overall memory usage to 272 = 72 + 200 = 72 + 25 x 8, hence enough to store up to 25 element references. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because a list is a referential structure, the result of getsizeof for a list instance only includes the size for representing its primary structurel it does not account for memory used by the objects that are elements of the list. In our experiment, we repeatedly append None to the list, because we do not care about the contents, but we could append any type of object without affecting the number of bytes reported by getsizeof(data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we were to continue such an experiment for further iterations, we might try to discern the pattern for how large of an array Python creates each time the capacity of the previous array is exhausted. Before exploring the precise sequence of capacities used by Python, we continue in this section by describing a general approach for implementing dynamic arrays and for performing an asymptotic analysis of their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Implementing a Dynamic Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the Python list class provides a highly optimized implementation of dynamic arrays, upon which we rely for the remainder of this book, it is instructive to see how such a class might be impmlemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key is to provide means to grow the array A that stores the elements of a list. Of course, we cannot actually grow that array, as its capacity is fixed. If an element is appeneded to a list at a time when the underlying array is full, we perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Allocate a new array B with larger capacity.\n",
    "2. Set B[i] = A[i] for i = 0, ..., n - 1 where n denotes current nubmer of items.\n",
    "3. Set A = B, that is, we henceforth use B as the array supporting the list.\n",
    "4. Insert the new element in the new array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An illustration of this process is shown in Figure 5.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The remaining issue to consider is how large of a new array to create. A commonly used rule is for the new array to have twice the capacity of the existing array that has been filled. In Section 5.3.2, we will provide a mathematical analysis to justufy such a choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Code Fragment 5.3, we offer a concrete implementation of dynamic arrays in Python. Our DynamicArray class is degined using ideas described in this section. While consistent with the interface of a Python list class, we provide only limited functionality in the form of an append method, and accessors \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_. Support for creating low-level arrays is provided by a module named ctypes. Because we will not typically use such a low-level structure in the remainder of this book, we omit a detailed explanation of  the ctypes module. Instead, we wrap the necessary commnad for declaring the raw array within a private utility method \\_make\\_array. The hallmark expansion procedure is performed in our nonpublic \\_resize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "class DynamicArray:\n",
    "    \"\"\"A dynamic array class akin to a simplified Python list.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Create an empty array.\"\"\"\n",
    "        self._n = 0\n",
    "        self._capacity = 1\n",
    "        self._A = self._make_array(self._capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of elements stored in the array.\"\"\"\n",
    "        return self._n\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Return element at index k.\"\"\"\n",
    "        if not 0 <= k < self._n:\n",
    "            raise IndexError('invalid index')\n",
    "        return self._A[k]\n",
    "    \n",
    "    def append(self, obj):\n",
    "        \"\"\"Add object to end of the array.\"\"\"\n",
    "        if self._n == self._capacity:\n",
    "            self._resize(2 * self._capacity)\n",
    "        self._A[self._n] = obj\n",
    "        self._n += 1\n",
    "        \n",
    "    def _resize(self, c):\n",
    "        \"\"\"Resize internal array to capacity c.\"\"\"\n",
    "        B = self._make_array(c)\n",
    "        for k in range(self._n):\n",
    "            B[k] = self._A[k]\n",
    "        self._A = B\n",
    "        self._capacity = c\n",
    "        \n",
    "    def _make_array(self, c):\n",
    "        \"\"\"Return new array with capacity c.\"\"\"\n",
    "        return (c * ctypes.py_object)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Amortized Analysis of Dynamic Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we perform a detailed analysis of the running time of operations on dynamic arrays. We use the big-Omega notation introduced in Section 3.3.1 to give an asymptotic lower bound on the running time of an algorithm or step within it.\n",
    "- The strategy of replacing an array with a new, larger array might at first seem slow, because a single append operation may require omega(n) time to perform, where n is the current number of elements in the array. However, notice that by doubling the capacity during an array replacement, our new array allows us to add n new elements before the array must be replaced again. In this way, there are many simple append operations for each expensive one. This fact allows us to show that performing a series of operations on an initially empty dynamic array is efficient in terms of its total running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using an algorithmic design pattern called amortization, we can show that performing a sequence of such append operations on a dynamic array is actually quite efficient. To perform an amortized analysis, we use an accounting technique where we view the computer as a coin-operated appliance that requires the payment of one cyber-dollar for a contant amount of computing time. When an operation is executed, we should have enough cyber-dollars available in our current \"bank account\" to pay for that operation's running time. Thus, the total amount of cyber-dollars spent for any computation will be proportional to the total time spent on that computation. The beauty of using this analysis method is that we can overcharge some operations in order to save up cyber-dollars to pay for others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Justification: Let us assume that one cyber-dollar is enough to pay for the execution of each append operation in S, exluding the time spent for growing the array. Also, let us assume that growing the array from size k to size 2k requires k cyber-dollars for the time spent initializing the new array. We shall charge each append operation three cyber-dollars. Thus, we overcharge each append operation that does not cause an overflow by two cyber-dollars. Think of the two cyber-dollars profited in an insertion that does not grow the array as being \"stored\" with the cell in which the element was inserted. An overflow occurs when the array S has 2^i elements, for some integer i >= 0, and the size of the array used by the array representing S is 2^i. Thus, doubling the size of the array will require 2^i cyber-dollars. Fortunately, these cyber-dollars can be found stored in cells 2^i-1 through 2^i - 1. Note that the previous overflow occurred when the number of elements becaome larger than 2^i-1 for the first time, and thus cyber-dollars stored in cells 2^i-1 through 2^i - 1 have not yet been spent. Therefore, we have a valid amoritzation scheme in which each operation is charged three cyber-dollars and all the computing time is paid for. That is, we can pay for the execution of n append operations using 3n cyber-dollars. In other words, the amortized running time of each append operations is  O(1); hence, the total running time of n append operations is O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Geometric Increase in Capacity **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the proof of Proposition 5.1 relies on the array being doubled each time we expand, the O(1) amortized bound per operation can be proven for any geometrically increasing progression of array sizes. When choosing the geometric base, there exists a trade-off between run-time efficiency and memory usage. With a base of 2, if the last insertion causes a resize event, the array essentially ends up twice as large as it needs to be. If we instead increase the  array by only 25% of its current size, we do not risk wasting as much memory in the end, but there will be more intermediate resize events along the way. Still it isi possible to prove an O(1) amortized bound, using a constant factor greater than the 3 cyber-dollars per operation used in the proof of Proposition 5.1. The key to the performance is that the amount of additional space is proportional to the current size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
