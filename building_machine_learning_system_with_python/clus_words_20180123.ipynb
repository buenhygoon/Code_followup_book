{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = \"./BuildingMachineLearningSystemsWithPython/ch03/data/toy\"\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distance Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorizer Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 100000\n",
    "best_i = None\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words on steroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The feature values simply count occurrences of terms in a post. We silently assumed that higher values for a term also mean that the term is of greater importance to the given post. But what about, for instance, the word \"subject\", which naturally occurs in each and every single post? Alright, we can tell CountVectorizer to remove it as well by means of its max_df parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **term frequency â€“ inverse document frequency (TF-IDF) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def tfidf(term, doc, corpus):\n",
    "    tf = doc.count(term) / len(doc)\n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = sp.log(len(corpus) / num_docs_with_term)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abb,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.270310072072\n",
      "0.135155036036\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))\n",
    "print(tfidf(\"b\", abb, D))\n",
    "print(tfidf(\"b\", abc, D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that a carries no meaning for any document since it is contained everywhere. The b term is more important for the document abb than for abc as it occurs there twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,\n",
    "                        self).build_analyzer()\n",
    "        return lambda doc: (\n",
    "            english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1,\n",
    "                                   stop_words='english',\n",
    "                                   decode_error='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our achievements and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> steps\n",
    "- firstly, tokenizing the text\n",
    "- This is followed by throwing away words that occur way too often to be of any help in detecting relevant posts\n",
    "- Throwing away words that occur way so seldoom so that there is only little changce that occur in future posts.\n",
    "- Counting the remaining words\n",
    "- Finally, calculating TF-IDF values from the counts, considering the whole text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drawbacks of *bag of words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It does not cover word relations\n",
    "- It does not capture negations correctly\n",
    "- It totally fails with misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Most clustering algorithms fall into one of the two methods: flat and hierarchical clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Flat clustering divides the posts into a set of clusters without relating the clusters to each other. The goal is simply to come up with a partitioning such that all posts in one cluster are most similar to each other while being dissimilar from the posts in all other clusters.** Many flat clustering algorithms require the number of clusters to be specified up front. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In hierarchical clustering, the number of clusters does not have to be specified. Instead, hierarchical clustering creates a hierarchy of clusters. While similar posts are grouped into one cluster, similar clusters are again grouped into one uber-cluster. This is done recursively, until only one cluster is left that contains everything. In this hierarchy, one can then choose the desired number of clusters after the fact. However, this comes at the cost of lower efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 news groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc', \\\n",
    "          'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',  \\\n",
    "          'comp.windows.x', 'sci.space']\n",
    "datasets = sklearn.datasets.fetch_20newsgroups(categories=groups)\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(\n",
    "    subset='train', categories=groups)\n",
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups) \n",
    "                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datasets.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n",
      "3529\n",
      "2349\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets.data))\n",
    "print(len(train_data.data))\n",
    "print(len(test_data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3529"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3529, 4712)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
    "                                   stop_words='english', decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5899.560\n",
      "Iteration  1, inertia 3218.298\n",
      "Iteration  2, inertia 3184.333\n",
      "Iteration  3, inertia 3164.867\n",
      "Iteration  4, inertia 3152.004\n",
      "Iteration  5, inertia 3143.111\n",
      "Iteration  6, inertia 3136.256\n",
      "Iteration  7, inertia 3129.325\n",
      "Iteration  8, inertia 3124.567\n",
      "Iteration  9, inertia 3121.900\n",
      "Iteration 10, inertia 3120.210\n",
      "Iteration 11, inertia 3118.627\n",
      "Iteration 12, inertia 3117.363\n",
      "Iteration 13, inertia 3116.811\n",
      "Iteration 14, inertia 3116.588\n",
      "Iteration 15, inertia 3116.417\n",
      "Iteration 16, inertia 3115.760\n",
      "Iteration 17, inertia 3115.374\n",
      "Iteration 18, inertia 3115.155\n",
      "Iteration 19, inertia 3114.949\n",
      "Iteration 20, inertia 3114.515\n",
      "Iteration 21, inertia 3113.937\n",
      "Iteration 22, inertia 3113.720\n",
      "Iteration 23, inertia 3113.548\n",
      "Iteration 24, inertia 3113.475\n",
      "Iteration 25, inertia 3113.447\n",
      "Converged at iteration 25: center shift 0.000000e+00 within tolerance 2.069005e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=50, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=3, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\n",
    "           verbose=1, random_state=3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ..., 41 14 16]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"\"\"\n",
    "Disk drive problems.\n",
    "Hi, I have a problem with my hard disk. \n",
    "After 1 year it is working only sporadically now. \n",
    "I tried to format it, but now it doesn't boot any more. \n",
    "Any ideas? Thanks\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "    \n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0378441731334072, \"From: Thomas Dachsel <GERTHD@mvs.sas.com>\\nSubject: BOOT PROBLEM with IDE controller\\nNntp-Posting-Host: sdcmvs.mvs.sas.com\\nOrganization: SAS Institute Inc.\\nLines: 25\\n\\nHi,\\nI've got a Multi I/O card (IDE controller + serial/parallel\\ninterface) and two floppy drives (5 1/4, 3 1/2) and a\\nQuantum ProDrive 80AT connected to it.\\nI was able to format the hard disk, but I could not boot from\\nit. I can boot from drive A: (which disk drive does not matter)\\nbut if I remove the disk from drive A and press the reset switch,\\nthe LED of drive A: continues to glow, and the hard disk is\\nnot accessed at all.\\nI guess this must be a problem of either the Multi I/o card\\nor floppy disk drive settings (jumper configuration?)\\nDoes someone have any hint what could be the reason for it.\\nPlease reply by email to GERTHD@MVS.SAS.COM\\nThanks,\\nThomas\\n+-------------------------------------------------------------------+\\n| Thomas Dachsel                                                    |\\n| Internet: GERTHD@MVS.SAS.COM                                      |\\n| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\\n| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\\n| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\\n| Fax:      +49 6221 415101                                         |\\n| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\\n| Tagline:  One bad sector can ruin a whole day...                  |\\n+-------------------------------------------------------------------+\\n\")\n"
     ]
    }
   ],
   "source": [
    "print(similar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another look at noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_group = zip(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n"
     ]
    }
   ],
   "source": [
    "all_ = [(len(post[0]), post[0], train_data.target_names[post[1]]) \\\n",
    "        for post in post_group]\n",
    "graphics = sorted([post for post in all_ if post[2] == 'comp.graphics'])\n",
    "print(graphics[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "noise_post = graphics[5][1]\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(\n",
    "    vectorizer.get_feature_names())\n",
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac)=3.51\n",
      "IDF(birmingham)=6.77\n",
      "IDF(host)=1.74\n",
      "IDF(kingdom)=6.68\n",
      "IDF(nntp)=1.77\n",
      "IDF(sorri)=4.14\n",
      "IDF(test)=3.83\n",
      "IDF(uk)=3.70\n",
      "IDF(unit)=4.42\n",
      "IDF(univers)=1.91\n"
     ]
    }
   ],
   "source": [
    "for term in sorted(useful):\n",
    "    print('IDF(%s)=%.2f' % (term,\n",
    "            vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, the terms with the highest discriminative power, birmingham and kingdom, clearly are not that computer graphics related, the same is the case with the terms with lower IDF scores. Understandably, posts from different newsgroups will be clustered together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P. 99 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
